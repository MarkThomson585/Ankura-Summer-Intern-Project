{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIREMENTS\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b958e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(column, word_list):\n",
    "    cleaned = []\n",
    "    processed_words = []\n",
    "    for word in word_list:\n",
    "        # Escape characters that have special meaning in regex\n",
    "        escaped_word = re.escape(word)\n",
    "\n",
    "        processed_words.append(escaped_word)\n",
    "    pattern = '|'.join(sorted(processed_words, key=len, reverse=True))\n",
    "    compiled_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "    for original_string in column:\n",
    "        if isinstance(original_string, str):\n",
    "            # Replace all occurrences of the words with an empty string\n",
    "            modified_string = compiled_pattern.sub('', original_string)\n",
    "            # Clean up extra spaces that might result from removal (e.g., \"Hello  World\")\n",
    "            modified_string = re.sub(r'\\s+', ' ', modified_string).strip()\n",
    "            cleaned.append(modified_string)\n",
    "        else:\n",
    "            # If the item is not a string (e.g., NaN, None), keep it as is\n",
    "            cleaned.append(original_string)\n",
    "    return cleaned\n",
    "\n",
    "remove_list = ['P.C.', '&', 'LLC', 'LLP', 'LAW', 'OFFICE', 'PLLC', 'P.A.', ' PC', 'Group', 'Legal', 'Aid', 'Offices', 'of',\n",
    "               'and', 'International', 'Inc.', 'P.L.L.C', 'the', 'Associates', 'PA ', 'APLC', 'L.L.P.', 'L.L.C.',\n",
    "               'P. A.', 'PLC', 'APC', 'firm', 'LPA', 'P.L.C', 'P.A', 'St. Louis', 'Pressional', 'Corp', \"LC\", '- MIAMI FL',\n",
    "               'ETC', '(Tulsa)', ' SC', '- Philadelphia', '(US)']\n",
    "\n",
    "def clean_text(text):\n",
    "   if not isinstance(text, str):\n",
    "       return \"\"\n",
    "   return re.sub(r'[^\\w\\s]', '', text.lower()).strip()\n",
    "# 2️⃣ Jaccard similarity: intersection / union of unique words\n",
    "def jaccard_similarity(a, b):\n",
    "   words_a = set(clean_text(a).split())\n",
    "   words_b = set(clean_text(b).split())\n",
    "   if not words_a and not words_b:\n",
    "       return 1.0  # Both empty\n",
    "   if not words_a or not words_b:\n",
    "       return 0.0  # One empty\n",
    "   return len(words_a & words_b) / len(words_a | words_b)\n",
    "# 3️⃣ Check if any b_entry matches a_entry with the threshold\n",
    "def has_match(a_entry, b_series, threshold=0.5):  # threshold can be tuned\n",
    "   for b_entry in b_series:\n",
    "       if jaccard_similarity(a_entry, b_entry) >= threshold:\n",
    "           return b_entry\n",
    "   return False\n",
    "# 4️⃣ Apply matching to lists in Column A\n",
    "def match_law_firm_lists(df, column_name, threshold=0.65):\n",
    "   AmLaw = pd.read_excel(\"C:/Users/mark.thomson/ML Proj/AmLaw 200 - Oct 24.xlsx\")\n",
    "   AmLaw_cleaned = remove_words(AmLaw['Firm Name'], remove_list)\n",
    "   def match_list(law_firm_list):\n",
    "       if not isinstance(law_firm_list, list):\n",
    "           return []\n",
    "       return [has_match(firm_name, AmLaw_cleaned, threshold) for firm_name in law_firm_list]\n",
    "   result_match = df[column_name].apply(match_list)\n",
    "   return result_match\n",
    "\n",
    "def clean_and_convert(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Remove '$' and ',' then convert to int\n",
    "        cleaned_value = value.replace('$', '').replace(',', '')\n",
    "        return int(cleaned_value)\n",
    "    except (ValueError, AttributeError):\n",
    "        # Handle cases where conversion might fail (e.g., if there are other unexpected strings)\n",
    "        return np.nan\n",
    "\n",
    "def find_matching_clients(parties_string, top_client_list):\n",
    "    \"\"\"\n",
    "    Finds elements in parties_string that contain any substring from top_client_list,\n",
    "    case-insensitively, using whole-word or boundary matching.\n",
    "\n",
    "    Args:\n",
    "        parties_string (str): A string of parties separated by ' / '.\n",
    "        top_client_list (list): A list of client substrings to search for.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of matching party elements, or False if no matches are found.\n",
    "    \"\"\"\n",
    "    if not isinstance(parties_string, str):\n",
    "        return False # Handle non-string entries if any\n",
    "\n",
    "    individual_parties = [p.strip() for p in parties_string.split('/')]\n",
    "    matching_clients = []\n",
    "\n",
    "    # Create regex patterns for each top client, ensuring whole word match and case-insensitivity\n",
    "    # For 'td', we want it to match 'td bank' but not 'ltd'.\n",
    "    # Using \\b (word boundary) or checking for specific delimiters will help.\n",
    "    # For simplicity and robust matching, using \\b with re.escape is a good approach.\n",
    "    regex_patterns = [re.compile(r'\\b' + re.escape(client) + r'\\b', re.IGNORECASE) for client in top_client_list]\n",
    "\n",
    "    for party in individual_parties:\n",
    "        for pattern in regex_patterns:\n",
    "            if pattern.search(party):\n",
    "                matching_clients.append(party) # Append the original party string\n",
    "                break  # Move to the next party once a match is found for the current party\n",
    "\n",
    "    return matching_clients if matching_clients else False\n",
    "\n",
    "top_client_list = ['Securities and Exchange Commission', \"SEC\", 'S.E.C.', 'JPMorgan', 'J.P.', 'Chase', 'Bank of America', 'bofa', 'Citigroup', 'Citibank', \n",
    "                       'citi', 'wells fargo', 'goldman sachs', 'Morgan Stanley', 'U.S. Bancorp', 'U.S. Bank', 'PNC', 'TD ', 'Capital One', 'C1', 'Charles Schwab', \n",
    "                       'BNY', 'The Bank of New York', 'State Street', 'BMO', 'American Express', 'Amex', 'UBS', 'M&T', 'Ameriprise', 'Santander', 'Northern Trust',\n",
    "                       'Deustche']\n",
    "\n",
    "def amlaw_match(df, AmLaw_path=0):\n",
    "    if AmLaw_path:\n",
    "        AmLaw = pd.read_excel(AmLaw_path)\n",
    "    else:\n",
    "        AmLaw = pd.read_excel(\"C:/Users/mark.thomson/ML Proj/AmLaw 200 - Oct 24.xlsx\")\n",
    "    \n",
    "    AmLaw_cleaned = remove_words(AmLaw['Firm Name'], remove_list)\n",
    "\n",
    "    cleaned_plaintiff = remove_words(df['plaintiff_attorney_firm'], remove_list)\n",
    "\n",
    "    cleaned_defendant = remove_words(df['defendant_attorney_firm'], remove_list)\n",
    "\n",
    "    plaint_split_clean = []\n",
    "\n",
    "    defend_split_clean = []\n",
    "\n",
    "    for item in cleaned_plaintiff:\n",
    "        if isinstance(item, str):\n",
    "            plaint_split_clean.append(item.split(' /'))\n",
    "        else:\n",
    "            plaint_split_clean.append(item)\n",
    "\n",
    "    df['cleaned_plaintiff'] = plaint_split_clean\n",
    "\n",
    "    for item in cleaned_defendant:\n",
    "        if isinstance(item, str):\n",
    "            defend_split_clean.append(item.split(' /'))\n",
    "        else:\n",
    "            defend_split_clean.append(item)\n",
    "\n",
    "    df['cleaned_defendant'] = defend_split_clean\n",
    "\n",
    "    df['plaintiff_matches'] = match_law_firm_lists(df, 'cleaned_plaintiff')\n",
    "\n",
    "    df['defendant_matches'] = match_law_firm_lists(df, 'cleaned_defendant')\n",
    "\n",
    "    df = df.drop(columns=['cleaned_plaintiff', 'cleaned_defendant'])\n",
    "\n",
    "    def false_check(list):\n",
    "        score = 0\n",
    "        for entry in list:\n",
    "            if entry != False:\n",
    "                score +=1\n",
    "        return score\n",
    "    df = df[(df['plaintiff_matches'].apply(false_check) > 0) | \\\n",
    "            (df['defendant_matches'].apply(false_check) > 0)]\n",
    "\n",
    "    df = df[(df['plaintiff_matches'].apply(len) > 0) | \\\n",
    "            (df['defendant_matches'].apply(len) > 0)]\n",
    "\n",
    "    df = df[['title', 'date_filed', 'last_updated', 'nature_of_suit', 'statute', 'docket_number', 'court', 'attorney', 'attorney_email', 'demand', 'parties', 'plaintiff_party',\n",
    "                 'defendant_party', 'defendant_attorney_firm', 'plaintiff_attorney_firm', 'plaintiff_matches', 'defendant_matches']]\n",
    "    \n",
    "    df['contains_top_client'] = df['parties'].apply(lambda x: find_matching_clients(x, top_client_list))\n",
    "\n",
    "    df['demand'] = df['demand'].apply(clean_and_convert)\n",
    "\n",
    "    df = df[\n",
    "    ( (df['demand'] == 75000) | (df['demand'] == 750000) | (df['demand'] >= 1000000) | (df['demand'].isna()) ) &\n",
    "    ( ~df['statute'].isin(['12:635 Breach of Insurance Contract', '28:1441 Petition for Removal- Breach of Contract']) )\n",
    "    ]\n",
    "\n",
    "\n",
    "    df_sorted = df.sort_values(by='contains_top_client', key=lambda x: x.apply(lambda val: val is not False), ascending=False)\n",
    "    \n",
    "    #print(df_sorted['docket_number'].str.cat(sep='\" OR \"'))\n",
    "\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd40035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legal_complaint_text_from_keyword(pdf_path, start_keyword=\"complaint\"):\n",
    "    \"\"\"\n",
    "    Extracts text from a legal complaint PDF file using pdfplumber,\n",
    "    starting from the first occurrence of a specified keyword (case-insensitive).\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the legal complaint PDF file.\n",
    "        start_keyword (str): The keyword to start extraction from (case-insensitive).\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text from the PDF, starting from the first occurrence\n",
    "             of the keyword. If the keyword is not found, an appropriate message\n",
    "             is printed, and an empty string is returned.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                # Add a separator for better readability between pages in the output\n",
    "                if page_num > 0:\n",
    "                    full_text += \"\\n--- Page {} ---\\n\\n\".format(page_num + 1)\n",
    "                full_text += page.extract_text()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The legal complaint file '{pdf_path}' was not found. Please check the path.\")\n",
    "        return \"\"\n",
    "    except pdfplumber.pdf.PDFError as e:\n",
    "        print(f\"Error opening or reading PDF '{pdf_path}': {e}\")\n",
    "        print(\"This might be due to a corrupted PDF or an unsupported PDF format.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during text extraction: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    demand_hooks = ['requested relief', 'prayer for relief', 'request for relief', 'prayer', 'prays for relief', 'relief requested',\n",
    "                    '\\nrequested relief\\n', '\\nprayer for relief\\n', '\\nrequest for relief\\n', '\\nprayer\\n', '\\nprays for relief\\n', '\\nrelief requested\\n',\n",
    "                     '\\nPRAYER FOR RELIEF\\n' ]\n",
    "\n",
    "\n",
    "    def safe_int_conversion(item):\n",
    "        try:\n",
    "            return int(item)\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    # Now, find the starting keyword (case-insensitive)\n",
    "    start_keyword_lower = start_keyword.lower()\n",
    "    full_text_lower = full_text.lower()\n",
    "    \n",
    "    high_confidence = False\n",
    "    largest_number = False\n",
    "    last_number = False\n",
    "    demand_info = []\n",
    "    hook_match = False\n",
    "\n",
    "    start_index = full_text_lower.find(start_keyword_lower)\n",
    "    stop_index_respect = full_text_lower.find('respectfully submitted')\n",
    "    stop_index_respect2 = full_text_lower.find('respedfully submitted')\n",
    "    stop_index_dated = full_text_lower.find('dated:')\n",
    "    if stop_index_dated != -1:\n",
    "        stop_index = stop_index_dated\n",
    "    elif stop_index_respect != -1:\n",
    "        stop_index = stop_index_respect\n",
    "    elif stop_index_respect2 != -1:\n",
    "        stop_index = stop_index_respect2\n",
    "    else:\n",
    "        stop_index = -1\n",
    "    complaint = 0\n",
    "    if start_index != -1:\n",
    "        # If found, return the text from that point onwards\n",
    "        complaint = full_text[start_index:stop_index]\n",
    "    else:\n",
    "        print(f\"The keyword '{start_keyword}' was not found in the legal complaint.\")\n",
    "        complaint = full_text # Or you could return full_text if you prefer to see everything when keyword is missing.\n",
    "\n",
    "\n",
    "    for hook in demand_hooks:\n",
    "        relief_index = full_text_lower.find(hook)\n",
    "        if relief_index != -1:\n",
    "            hook_match = True\n",
    "            relief_text = full_text_lower[relief_index:stop_index]\n",
    "            \n",
    "            pattern = r\"\\$\\s*([\\d,]+)\"\n",
    "\n",
    "            # Find all matches\n",
    "            matches = re.findall(pattern, relief_text)\n",
    "\n",
    "            # Remove commas from the matched strings\n",
    "            matches = [number.replace(',', '') for number in matches]\n",
    "    \n",
    "    # Convert the list of strings to integers\n",
    "            if matches:\n",
    "                \n",
    "                numbers = [safe_int_conversion(match) for match in matches]\n",
    "\n",
    "                if numbers:\n",
    "                    largest_number = max(numbers)\n",
    "                    last_number = numbers[-1]\n",
    "                else:\n",
    "                    largest_number = False\n",
    "                    last_number = False\n",
    "                high_confidence = True\n",
    "                demand_info = [largest_number, last_number, high_confidence]\n",
    "            else:\n",
    "                hook_match = False\n",
    "    if not hook_match:\n",
    "\n",
    "        pattern = r\"\\$\\s*([\\d,]+)\"\n",
    "        # Find all matches\n",
    "        matches = re.findall(pattern, complaint)\n",
    "        # Remove commas from the matched strings\n",
    "        matches = [number.replace(',', '') for number in matches]\n",
    "        numbers = [safe_int_conversion(match) for match in matches]\n",
    "\n",
    "        if numbers:\n",
    "            largest_number = max(numbers)\n",
    "            last_number = numbers[-1]\n",
    "        else:\n",
    "            largest_number = False\n",
    "            last_number = False\n",
    "        demand_info = [largest_number, last_number, high_confidence]\n",
    "            \n",
    "    return [complaint, largest_number, last_number, high_confidence]\n",
    "\n",
    "    # Find the index of the first occurrence of the keyword\n",
    "    \n",
    "positive_words = ['ponzi scheme', 'venture capital', 'private equity', 'broker dealer', 'fraudulent digital asset trading', 'misappropriated funds', \n",
    "                  'mutual fund investigation', 'dually registered personnel', 'misleading investors', 'intellectual property', 'contract dispute', 'master limited partnership', \n",
    "                  'algorithimic trading', 'exchange traded fund', 'financial arbitrage', 'venture capital funds', 'fee structure', 'financial bonds','financial bond', 'breach of contract', \n",
    "                  'annuities', 'financial futures', 'futures contracts', 'pump and dump', 'sovereign bonds', 'repo financing', 'RMBS', 'residential mortgage-backed security', 'CDO Valuation', \n",
    "                  'collateralized debt obligation', 'mortage backed security', 'financial due diligence', 'asset classification', 'financial indentures', 'certificates of deposit', \n",
    "                  'special purpose vehicle', 'asset-backed security', 'unauthorized transfer', 'fiduciary duty', 'promissory note']\n",
    "\n",
    "negative_words = ['Crypto', 'Precious Metals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_strings_as_dict(folder_path):\n",
    "    \"\"\"\n",
    "    Converts all PDF files in a given folder into a dictionary,\n",
    "    where keys are filenames (without extension) and values are PDF contents as strings.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping PDF filenames (e.g., \"report1\") to their string content.\n",
    "              Returns an empty dictionary if no PDFs are found or if the folder doesn't exist.\n",
    "    \"\"\"\n",
    "    pdf_dict = {}\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: Folder not found at '{folder_path}'\")\n",
    "        return {}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(filepath) and filename.lower().endswith('.pdf'):\n",
    "            content_string = extract_legal_complaint_text_from_keyword(filepath)\n",
    "            if content_string is not None:\n",
    "                # Get the filename without the extension to use as the key\n",
    "                file_name_without_extension = os.path.splitext(filename)[0]\n",
    "                pdf_dict[file_name_without_extension] = content_string\n",
    "\n",
    "    return pdf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FinBERT model and tokenizer\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "def get_finbert_embedding(text):\n",
    "    \"\"\"\n",
    "    Generates a document embedding for the given text using FinBERT.\n",
    "    Uses mean pooling of token embeddings.\n",
    "    \"\"\"\n",
    "    # Tokenize the preprocessed text for FinBERT\n",
    "    inputs = finbert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass output_hidden_states=True to get the hidden states\n",
    "        outputs = finbert_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Access the last hidden state from the outputs\n",
    "    # outputs.hidden_states is a tuple of all hidden states\n",
    "    # outputs.hidden_states[-1] is the last hidden state\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "    # Use mean pooling of the last hidden states to get a single document embedding\n",
    "    # The dimensions are [batch_size, sequence_length, hidden_size]\n",
    "    document_embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    return document_embedding\n",
    "\n",
    "# Example usage (assuming 'positive_words' and 'negative_words' are defined lists)\n",
    "\n",
    "\n",
    "positive_keyword_embeddings = {\n",
    "    kw: get_finbert_embedding(kw) for kw in positive_words\n",
    "}\n",
    "negative_keyword_embeddings = {\n",
    "    kw: get_finbert_embedding(kw) for kw in negative_words\n",
    "}\n",
    "\n",
    "positive_keyword_embeddings = {k: v for k, v in positive_keyword_embeddings.items() if v is not None}\n",
    "negative_keyword_embeddings = {k: v for k, v in negative_keyword_embeddings.items() if v is not None}\n",
    "\n",
    "print(\"Positive keyword embeddings generated.\")\n",
    "print(\"Negative keyword embeddings generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ced5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "SEMANTIC_SIMILARITY_THRESHOLD = 0.85 # Adjust this threshold (0.0 to 1.0)\n",
    "\n",
    "def get_recommendation_score_finbert_keywords(new_complaint_text):\n",
    "    # Process with spaCy to get individual tokens and their base forms\n",
    "    doc = nlp(new_complaint_text)\n",
    "    lemmas = []\n",
    "    token_num = len(doc)\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemmas.append(token.lemma_.lower().strip())\n",
    "\n",
    "    lemma_counts = Counter(lemmas)\n",
    "\n",
    "    unique_lemma = list(lemma_counts.keys())\n",
    "    print(len(unique_lemma))\n",
    "    # Get FinBERT embedding for the entire document for overall financial sentiment\n",
    "    # Note: ProsusAI/finbert is a sentiment model, so we can use its classifier head if needed.\n",
    "    # For now, we'll just use its embeddings.\n",
    "    # If you want direct sentiment:\n",
    "    # finbert_sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    # finbert_sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "    # inputs = finbert_sentiment_tokenizer(new_complaint_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # with torch.no_grad():\n",
    "    #     logits = finbert_sentiment_model(**inputs).logits\n",
    "    # probabilities = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    # sentiment_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "    # financial_sentiment_score = probabilities[sentiment_labels.index(\"negative\")] # Or positive\n",
    "\n",
    "    # For Option B, we are focusing on keyword/semantic overlap for score,\n",
    "    # rather than directly using FinBERT's sentiment head.\n",
    "\n",
    "    pos_match_count = 0\n",
    "    neg_match_count = 0\n",
    "    financial_match_count = 0\n",
    "    \n",
    "    # Track which keywords have already been matched to avoid double counting on similar tokens\n",
    "    matched_pos_keywords = set()\n",
    "    matched_neg_keywords = set()\n",
    "\n",
    "\n",
    "    for lemma in unique_lemma:\n",
    "        #lemma = token.lemma_.lower().strip()\n",
    "        #if not lemma or not token.has_vector: # Check if spacy has a vector for the token\n",
    "            #continue\n",
    "\n",
    "        # Get FinBERT embedding for the current token's lemma (or raw text)\n",
    "        # We need to get a FinBERT embedding for each token to compare it\n",
    "        # with the FinBERT embeddings of our keywords.\n",
    "        # This is where FinBERT's strength comes in for nuance.\n",
    "        token_finbert_embedding = get_finbert_embedding(lemma)\n",
    "        if token_finbert_embedding is None:\n",
    "            continue\n",
    "\n",
    "        # Check for semantic similarity with positive keywords\n",
    "        for pk_text, pk_embed in positive_keyword_embeddings.items():\n",
    "            if 1 - cosine(token_finbert_embedding, pk_embed) > SEMANTIC_SIMILARITY_THRESHOLD:\n",
    "                pos_match_count += lemma_counts[lemma]\n",
    "                matched_pos_keywords.add(pk_text) # Mark keyword as matched\n",
    "                print(lemma)\n",
    "                print(pk_text)\n",
    "                print(1- cosine(token_finbert_embedding, pk_embed))\n",
    "                print(\"-----\")\n",
    "                break # Move to next token, this token found a positive match\n",
    "\n",
    "        # Check for semantic similarity with negative keywords\n",
    "        '''\n",
    "        for nk_text, nk_embed in negative_keyword_embeddings.items():\n",
    "            if 1 - cosine(token_finbert_embedding, nk_embed) > SEMANTIC_SIMILARITY_THRESHOLD:\n",
    "                neg_match_count += lemma_counts[lemma]\n",
    "                print(lemma)\n",
    "                print(nk_text)\n",
    "                print(1- cosine(token_finbert_embedding, nk_embed))\n",
    "                print(\"-----\")\n",
    "                matched_neg_keywords.add(nk_text)\n",
    "                break\n",
    "        '''\n",
    "\n",
    "    # --- Score Calculation Logic (Crucial and highly customizable) ---\n",
    "    # This is a simple weighted sum. You'll need to define weights based on domain expertise.\n",
    "    # Higher negative impact, medium financial relevance, lower positive impact in this example.\n",
    "    if token_num != 0:\n",
    "        return [pos_match_count / token_num, pos_match_count, neg_match_count / token_num, neg_match_count]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(filepath):\n",
    "    corpus = get_pdf_strings_as_dict(filepath)\n",
    "    corpus2 = pd.DataFrame(list(corpus.items()), columns=['Case Key', 'Info'])\n",
    "    new_cols = corpus2['Info'].apply(pd.Series)\n",
    "\n",
    "# Rename the new columns for clarity\n",
    "    new_cols.columns = ['Comaplaint', 'Largest Number', 'Last Number', 'Demand Confidence']\n",
    "\n",
    "# Concatenate the new columns to the original DataFrame\n",
    "    corpus3 = pd.concat([corpus2, new_cols], axis=1)\n",
    "\n",
    "# Drop the original 'Value_Column'\n",
    "    corpus3 = corpus3.drop('Info', axis=1)\n",
    "    return corpus3\n",
    "\n",
    "def remove_false_items(input_list):\n",
    "    \"\"\"\n",
    "    Returns a new list with all False values removed.\n",
    "    \"\"\"\n",
    "    return [item for item in input_list if item is not False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER ORIGINAL DATA FROM BLOOMBERG LAW HERE (CHANGE FILE PATH)\n",
    "raw_bloombeg_data = pd.read_csv(\"C:/Users/mark.thomson/ML Proj/bloomberg_law_docket_search_results_2025-08-07.csv\")\n",
    "phase1_blaw_data = amlaw_match(raw_bloombeg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE FILE PATH TO WHERE YOU WANT THE PHASE 1 OUTPUT TO BE\n",
    "phase1_blaw_data.to_csv(\"C:/Users/mark.thomson/ML Proj/phase_1_2025-08-07 v6.csv\", index=True, index_label='case_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER FOLDER OF COMPLAINTS HERE\n",
    "new_corpus = get_corpus('C:/Users/mark.thomson/ML Proj/complaints 8-7-25')\n",
    "embedding_scores = new_corpus['Comaplaint'].apply(get_recommendation_score_finbert_keywords)\n",
    "new_corpus['Positive Percent'] = [embedding_scores[i][0] for i in range(len(embedding_scores))]\n",
    "new_corpus['Positive Count'] = [embedding_scores[i][1] for i in range(len(embedding_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTER MODIFIED PHASE 1 OUTPUT HERE\n",
    "final_part = pd.read_excel(\"C:/Users/mark.thomson/ML Proj/phase_1_2025-08-07 v6-filled.xlsx\")\n",
    "final_part['case_key'] = [str(i) for i in final_part['case_key']]\n",
    "final = pd.merge(final_part, new_corpus, on='case_key', how='right')\n",
    "final['securities fraud class action'] = [1 if ('class action' in i or 'class-action' in i) and '10b-5' in i else 0 for i in final['Comaplaint']]\n",
    "#These two lines were not tested, might cause errors\n",
    "final['plaintiff_matches'] = final['plaintiff_matches'].apply(remove_false_items)\n",
    "final['defendant_matches'] = final['defendant_matches'].apply(remove_false_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128877a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE FILE PATH TO WHERE YOU WANT FINAL OUTPUT TO BE\n",
    "final.to_excel('C:/Users/mark.thomson/ML Proj/8-12-25 demo v2.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
